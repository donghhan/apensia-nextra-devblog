# 단어 임베딩

단어 임베딩이란 <ins>**분포 가설을 기반으로 하여**</ins> 단어의 의미를 간직하는 <ins>**밀집 벡터(Dense Vector)**</ins>로 표현하는 방법을 말한다.
단어 임베딩에는 크게 극소 표현과 [분산 표현](#분산-표현) 방법이 있다.

## 원-핫 인코딩 한계 극복

### 공간 비효율성

앞서 원-핫 인코딩 방식에서는 단어들이 새로 추가가 될 때마다 차원의 수가 증가하게 되기 때문에 공간의 비효율성이 발생한다는 문제점을 가지고 있었다.
단어 임베딩 형식에서는 이를 어떻게 극복하는지 살펴보자.

<br />

<center>
  <figure>
    <img
      src="https://d33wubrfki0l68.cloudfront.net/429d58bc351cd28595d66b720acf07eb8f194af8/0ff77/images/nlp-embedding-methods-3.jpg"
      alt="Dense Vector"
      width="500"
    />
    <figcaption>
      출처:
      [Pinecone](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/)
    </figcaption>
  </figure>
</center>

위의 그림에서처럼 2차원 형태의 공간에서 각 단어들의 벡터가 실수 형태로 들어가게 되고, 이에 따라 유사성이 있는 단어들끼리는 **밀집**하여 데이터가 만들어지게 된다.
원-핫 인코딩 방식과는 다르게 단어 임베딩 형식에서는 벡터가 한 공간에 꽉 차있는 형태로 이루어져 있기 때문에 새로운 단어가 추가가 되더라도 차원을 굳이 따로 추가할 필요는 없다.
이에 따라 불필요하게 공간이 낭비되는 것을 방지할 수 있고 추후 분류나 예측 모델을 학습할 때 연산 과정을 한층 더 줄일 수 있는 이점을 가지고 있다.

### 단어의 의미

원-핫 인코딩에서는 단어의 의미의 유사성을 제대로 표현하지 못한다는 또다른 단점이 있었다. 단어 임베딩에서는 이를 어떻게 해결할까?

맨 위의 정의에서 <b>분포 가설(Distributional Hypothesis)</b>이란 <ins>_같은 문맥에서 등장하는 단어는 유사한 의미를 지닌다_</ins>는 가설을 지칭한다.
단어 임베딩의 방식 상에서는 위의 분포 가설을 기반으로 하여 단어들의 밀집 벡터들을 생성하게 된다.

<br />

<center>
  <figure>
    <img
      src="https://www.researchgate.net/publication/352177721/figure/fig1/AS:1031905331912708@1623036811743/Illustration-of-the-Distributional-Hypothesis-and-Language-Model-Distance-The-accuracy.png"
      alt="Distributional Hypothesis"
      width="500"
    />
    <figcaption>
      출처: [Research
      Gate](https://www.researchgate.net/figure/Illustration-of-the-Distributional-Hypothesis-and-Language-Model-Distance-The-accuracy_fig1_352177721)
    </figcaption>
  </figure>
</center>

위의 예시에서 person, woman, man, camera, tv 등 여러 단어들의 벡터가 임의의 위치에 생성이 되고, 이후 분류 모델을 통해 학습이 진행될수록 같은 문맥(혹은 문장) 내에서 등장하는 단어들은 코사인 유사도가 더욱 높아지는 방향으로
재배치되게 된다.

## 분산 표현

<b>분산 표현(Distributed Representation)</b>이란 <ins>
  <i>단어를 표현하기 위해 주변을 참고</i>
</ins>하는 방법을 지칭한다. 분산 표현에 기반한 단어 임베딩 방법론이 제시된 이후로
빈도 기반의 단어 임베딩 방법론들은 거의 사라지게 되었다. 각 차원의 의미를 해석하기는
어려워졌으나 다양한 태스크 속에서 강력한 성능을 보이고 있다.

예를 들어, 바나나는 "노랗다", "달다", "길다" 등의 단어가 주로 함께 등장하게 되는데 분포 가설에 따라서 해당 내용을 가진 텍스트의 단어들을 벡터화한다면 해당 단어 벡터들은 유사한 벡터들을 가지게 된다.
