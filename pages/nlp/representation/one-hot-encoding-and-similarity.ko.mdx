# 원-핫 인코딩 & 유사도 계산

## 원-핫 인코딩

단어의 표현 중 간단한 방법론 중 하나이다. 글자 그대로 알 수 있듯이 단어에 대한 차원을 생성한 후, 그 단어가 들어갈 공간을 `1`로, 이외는 전부 `0`으로 표현하는 방법이다.

예를 들어, _한국_, _미국_, _태국_ 이렇게 세 단어가 있다고 가정했을 때

```py
한국 = [1, 0, 0]
미국 = [0, 1, 0]
태국 = [0, 0, 1]
```

이렇게 표현을 하는 것을 의미한다.

### 한계

하지만 원-핫 인코딩에는 몇 가지 한계점이 있다.

#### 공간의 비효율성

위의 예제에서 유추해볼 수 있듯이 단어의 개수가 늘어나면 늘어날수록 차원의 개수도 늘어나게 된다.
따라서, **단어의 수만큼 차원이 필요하게 되므로**, 저장 공간이 불필요하게 낭비되고 연산 과정도 복잡해진다.

```py
한국 = [1, 0, 0, 0]
미국 = [0, 1, 0, 0]
태국 = [0, 0, 1, 0]
일본 = [0, 0, 0, 1]
# ...
```

#### 의미의 표현 한계

또한 의미를 제대로 담지 못하는 문제도 있다. 예를 들어

- 원숭이
- 사과
- 바나나
- 개
- 고양이

이렇게 5개의 단어가 있다고 가정했을 때,

1. 원숭이와 개 그리고 고양이는 *동물*이라는 하나의 카테고리로 묶일 수 있고,
2. 사과와 바나나는 *과일*이라는 하나의 카테고리로 묶일 수 있으며,
3. 원숭이와 바나나는 *원숭이는 바나나를 좋아한다*는 관계로 묶일 수 있다.

하지만 원-핫 인코딩 방식을 따르게 되면 이와 같이 **단어의 유사성**을 토대로 단어들을 묶어서 생각하기 힘들게 한다.

<br />

## 유사도 계산

### 유클리디언 거리

단어들을 벡터로 표현을 하고 그 벡터들간의 거리를 측정할 수가 있는데, 이 때 이 거리를 유클리디언 거리라 한다.

<br />

<center>
  <figure>
    <img
      src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/10/Euclidean_distance_3d_2_cropped.png/1024px-Euclidean_distance_3d_2_cropped.png"
      alt="Euclidiean Distance"
      width="500"
    />
    <figcaption>출처: Wikipedia</figcaption>
  </figure>
</center>

### 자카드 유사도

문서 혹은 문장 간의 유사도를 측정할 때 많이 사용하는 방법이다.

<br />

<center>$J(A,B) = \cfrac{| A \bigcap B |}{|A \bigcup B|} = \cfrac{| A \bigcap B |}{|A| + |B| - |A \bigcap B|}$</center>

<br />

<center>
  <figure>
    <img
      src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1f/Intersection_of_sets_A_and_B.svg/1024px-Intersection_of_sets_A_and_B.svg.png"
      alt="Jaccard Index"
      width="300"
    />
    <figcaption>출처: Wikipedia</figcaption>
  </figure>
</center>

### 코사인 유사도

두 벡터간의 유사도를 측정하는 방법 중 하나이며 두 벡터 사이의 코사인을 측정한다. 1에 가까울수록 유사도가 높으며 유사도가 높다는 것은 유사한 의미를 가진다는 것을 의미한다.

<center>$\cos(\theta) = \cfrac{A \cdot B}{ \Vert A \Vert \Vert B \Vert} = \cfrac{\displaystyle\sum_{i=1}^n A_i \times B_i }{\sqrt{\displaystyle\sum_{i=1}^n (A_i)^2} \times \sqrt{\displaystyle\sum_{i=1}^n (B_i)^2}}$</center>

<br />

<center>
  <figure>
    <img
      src="https://media.licdn.com/dms/image/C4D12AQGLuiuropsWUA/article-cover_image-shrink_720_1280/0/1520203391934?e=2147483647&v=beta&t=0q8pImQxS1toIpFrnF9FIdWEsOQ1eUmZF_PGr50bJUY"
      alt="Cosine Similarity"
      width="500"
    />
    <figcaption>
      출처: [Chen
      Yang](https://www.linkedin.com/pulse/machine-learning-intuition-cosine-similarity-chen-yang)
    </figcaption>
  </figure>
</center>

> 원-핫 인코딩 방식으로 단어들을 좌표평면에 나열했을 경우 대부분 직교하는 형태로 나타나게 되는데 이를 코사인 유사도를 통해서 값을 구해보면 항상 0의 값이 나오는 것을 알 수 있다.
> 즉, 원-핫 벡터간 코사인 유사도는 모두 0이 나오기 때문에 의미를 분간하기 어렵다.
