# Word2Vec

분산 표현에 기반한 단어 임베딩의 학습 방법 중 가장 대표적인 방법이 바로 Word2Vec이다.

<br />

<center>
  <figure>
    <img
      src="https://www.researchgate.net/publication/334209824/figure/fig3/AS:776808106577938@1562216893819/Word2vec-skip-gram-and-analogies-a-Target-words-LiCoO2-and-LiMn2O4-are-represented.png"
      alt="Word2Vec"
      width="700"
    />
    <figcaption>
      출처: [Research
      Gate](https://www.researchgate.net/figure/Word2vec-skip-gram-and-analogies-a-Target-words-LiCoO2-and-LiMn2O4-are-represented_fig3_334209824)
    </figcaption>
  </figure>
</center>

## 정의

[논문](https://arxiv.org/pdf/1301.3781.pdf) 상의 초록 부분을 한 번 살펴보도록 하자.

> We propose <ins>**two novel model architectures for computing continuous vector representations of words**</ins> from very large data sets. <ins>**The quality of these representations
> is measured in a word similarity task**</ins>, and the results are compared to the previously best performing techniques based on different types of neural networks. We
> observe large improvements in <ins>**accuracy at much lower computational cost**</ins>, i.e. it
> takes less than a day to learn high quality word vectors from a 1.6 billion words
> data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.

요약을 하자면 Word2Vec 모델은

1. 단어의 연속적 벡터 표현으로써 2가지 모델을 제안하고 있는데, 이 모델들은
2. 단어 유사성으로 벡터 표현의 질을 측정하며
3. 더 적은 비용으로 높은 정확도를 개선하고 있다

고 한다. 앞서 원-핫 인코딩에서 단어의 유사성을 코사인 유사도를 통해 측정하는 방식과는 약간 다르게 단어의 유사성이 측정되고 있다.

> Many current NLP systems and techniques <ins>**treat words as atomic units**</ins> - there is <ins>**no notion of similarity between words**</ins>, as these are represented as indices in a vocabulary. This choice has several good
> reasons - simplicity, robustness and the observation that simple models trained on huge amounts of
> data outperform complex systems trained on less data...
> With progress of machine learning techniques in recent years, it has become <ins>**possible to train more
> complex models on much larger data set**</ins>, and they typically outperform the simple models. Probably
> the most successful concept is to use distributed representations of words [10]. For example, neural
> network based language models significantly outperform N-gram models [1, 27, 17].

예전의 경우 단어를 하나의 원자 단위로 취급하였으며 각 단어들 간의 유사성에 대한 개념이 없었다. 그리고 많은 양의 데이터를 활용한 단순 모델 학습이 적은 데이터를 활용한 복잡한 모델 학습보다 성능이 더 좋았다.
하지만 시간이 점차 지나 기술이 발전함에 따라 이제는 많은 양의 데이터를 복잡한 모델로 학습시키는 것이 가능해졌고, 이 과정에서 단어의 분산 표형(Distributed Representation)을 사용하게 된다.

> The main goal of this paper is to introduce techniques that can be used for <ins>**learning high-quality word
> vectors from huge data sets with billions of words**</ins>, and with millions of words in the vocabulary...
> We use recently proposed techniques for measuring the quality of the resulting vector representations, with the expectation that not only will <ins>**similar words tend to be close to each other, but that
> words can have multiple degrees of similarity.**</ins>

Word2Vec 모델을 통해 이루고자 하는 주 목표는 수많은 단어로 질 좋은(high-quality) 단어 벡터를 학습시키는 것인데, 여기서 말하는 _질 좋은_ 데이터란 유사 단어 간에는 거리가 가까운 경향이 있고 단어들은 다양한 유사도를 가지
